{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kE_89Zf9qJH0",
    "outputId": "0e7d77b4-edeb-4008-86b1-6fca71019184"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "path = './../data/'\n",
    "\n",
    "users_df = pd.read_csv(path + 'users.csv')\n",
    "suggestions_df = pd.read_csv(path + 'suggestions.csv')\n",
    "jbsteps_df = pd.read_csv(path + 'jbsteps.csv')\n",
    "gfsteps_df = pd.read_csv(path + 'gfsteps.csv')\n",
    "\n",
    "# print(users_df.head())\n",
    "# print(suggestions_df.head())\n",
    "# print(jbsteps_df.head())\n",
    "# print(gfsteps_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUlf0rKOsUgO"
   },
   "outputs": [],
   "source": [
    "# 1. Checking for missing values across all datasets\n",
    "missing_values_suggestions = suggestions_df.isnull().sum()\n",
    "missing_values_users = users_df.isnull().sum()\n",
    "missing_values_jbsteps = jbsteps_df.isnull().sum()\n",
    "missing_values_gfsteps = gfsteps_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-PGIjC227z7",
    "outputId": "597449a9-f60c-443e-f9d5-3c5fba688b83"
   },
   "outputs": [],
   "source": [
    "## Print missing values summary\n",
    "print(\"Missing Values in Suggestions Dataset:\")\n",
    "print(missing_values_suggestions)\n",
    "print(\"\\nMissing Values in Users Dataset:\")\n",
    "print(missing_values_users)\n",
    "print(\"\\nMissing Values in JBSteps Dataset:\")\n",
    "print(missing_values_jbsteps)\n",
    "print(\"\\nMissing Values in GFSteps Dataset:\")\n",
    "print(missing_values_gfsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvxNIndX59p4"
   },
   "outputs": [],
   "source": [
    "# 2. Assessing column names\n",
    "columns_suggestions = suggestions_df.columns\n",
    "columns_users = users_df.columns\n",
    "columns_jbsteps = jbsteps_df.columns\n",
    "columns_gfsteps = gfsteps_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSozJS5Y6Cxw",
    "outputId": "d4da4297-194b-4a0f-8aff-902c3b6d69ea"
   },
   "outputs": [],
   "source": [
    "# Print column names\n",
    "print(\"\\nColumn Names in Suggestions Dataset:\")\n",
    "print(columns_suggestions)\n",
    "print(\"\\nColumn Names in Users Dataset:\")\n",
    "print(columns_users)\n",
    "print(\"\\nColumn Names in JBSteps Dataset:\")\n",
    "print(columns_jbsteps)\n",
    "print(\"\\nColumn Names in GFSteps Dataset:\")\n",
    "print(columns_gfsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pUsPswA6FpS"
   },
   "outputs": [],
   "source": [
    "# 3. Checking data completeness by calculating the percentage of missing values\n",
    "data_completeness_suggestions = suggestions_df.isnull().mean() * 100\n",
    "data_completeness_users = users_df.isnull().mean() * 100\n",
    "data_completeness_jbsteps = jbsteps_df.isnull().mean() * 100\n",
    "data_completeness_gfsteps = gfsteps_df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fms4vNTs6Iai",
    "outputId": "edf55ec5-b611-4fa2-d0e7-01280abb14c1"
   },
   "outputs": [],
   "source": [
    "# Print data completeness\n",
    "print(\"\\nData Completeness in Suggestions Dataset (% of missing values):\")\n",
    "print(data_completeness_suggestions)\n",
    "print(\"\\nData Completeness in Users Dataset (% of missing values):\")\n",
    "print(data_completeness_users)\n",
    "print(\"\\nData Completeness in JBSteps Dataset (% of missing values):\")\n",
    "print(data_completeness_jbsteps)\n",
    "print(\"\\nData Completeness in GFSteps Dataset (% of missing values):\")\n",
    "print(data_completeness_gfsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzrHSs_i6Pl8"
   },
   "outputs": [],
   "source": [
    "# Step 1: Handling Missing Values\n",
    "# We will handle missing values by either removing or imputing them based on the percentage of missing data.\n",
    "\n",
    "# Remove columns with more than 40% missing data in each dataset\n",
    "threshold = 0.4\n",
    "\n",
    "suggestions_cleaned = suggestions_df.loc[:, suggestions_df.isnull().mean() < threshold]\n",
    "users_cleaned = users_df.loc[:, users_df.isnull().mean() < threshold]\n",
    "jbsteps_cleaned = jbsteps_df.loc[:, jbsteps_df.isnull().mean() < threshold]\n",
    "gfsteps_cleaned = gfsteps_df.loc[:, gfsteps_df.isnull().mean() < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GaQpZbB74Xa",
    "outputId": "9a75e920-8360-46d4-945b-6224eb65cf47"
   },
   "outputs": [],
   "source": [
    "print(\"\\n Cleaned suggestions data \")\n",
    "print(suggestions_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOh2yE-n8HXg",
    "outputId": "4e4eca6b-2662-482f-8f42-d6f2ed6606da"
   },
   "outputs": [],
   "source": [
    "# For columns with less missing data, we will impute the missing values with the median (for numerical) and mode (for categorical).\n",
    "# Handle missing values explicitly using .loc\n",
    "suggestions_cleaned.loc[:, :] = suggestions_cleaned.fillna(suggestions_cleaned.median(numeric_only=True))\n",
    "suggestions_cleaned.loc[:, :] = suggestions_cleaned.fillna(suggestions_cleaned.mode().iloc[0])\n",
    "\n",
    "users_cleaned.loc[:, :] = users_cleaned.fillna(users_cleaned.median(numeric_only=True))\n",
    "users_cleaned.loc[:, :] = users_cleaned.fillna(users_cleaned.mode().iloc[0])\n",
    "\n",
    "jbsteps_cleaned.loc[:, :] = jbsteps_cleaned.fillna(jbsteps_cleaned.median(numeric_only=True))\n",
    "jbsteps_cleaned.loc[:, :] = jbsteps_cleaned.fillna(jbsteps_cleaned.mode().iloc[0])\n",
    "\n",
    "gfsteps_cleaned.loc[:, :] = gfsteps_cleaned.fillna(gfsteps_cleaned.median(numeric_only=True))\n",
    "gfsteps_cleaned.loc[:, :] = gfsteps_cleaned.fillna(gfsteps_cleaned.mode().iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOx_mDH-8YtU",
    "outputId": "422aa2eb-d731-48f9-fa2c-2ae6bf5a13ce"
   },
   "outputs": [],
   "source": [
    "# Step 2: Data Type Validation\n",
    "# Convert date/time columns to datetime format if not already\n",
    "\n",
    "# Convert in suggestions dataset\n",
    "\n",
    "if 'sugg.select.utime' in suggestions_cleaned.columns:\n",
    "    suggestions_cleaned.loc[:, 'sugg.select.utime'] = pd.to_datetime(suggestions_cleaned['sugg.select.utime'], errors='coerce')\n",
    "\n",
    "# Convert in jbsteps dataset\n",
    "if 'steps.utime' in jbsteps_cleaned.columns:\n",
    "    jbsteps_cleaned.loc[:, 'steps.utime'] = pd.to_datetime(jbsteps_cleaned['steps.utime'], errors='coerce')\n",
    "\n",
    "# Convert in gfsteps dataset\n",
    "if 'steps.utime' in gfsteps_cleaned.columns:\n",
    "    gfsteps_cleaned.loc[:, 'steps.utime'] = pd.to_datetime(gfsteps_cleaned['steps.utime'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_i0ZVlOI826O",
    "outputId": "624c0a52-438a-4837-9c2f-12556bca23c7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Displaying the shape of cleaned data after handling missing values and data type corrections\n",
    "cleaned_data_shapes = {\n",
    "    \"Suggestions Cleaned Shape\": suggestions_cleaned.shape,\n",
    "    \"Users Cleaned Shape\": users_cleaned.shape,\n",
    "    \"JBSteps Cleaned Shape\": jbsteps_cleaned.shape,\n",
    "    \"GFSteps Cleaned Shape\": gfsteps_cleaned.shape\n",
    "}\n",
    "\n",
    "cleaned_data_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Yy61gr69vut"
   },
   "source": [
    "***Feature Engineering:***\n",
    "We’ll focus on aggregating step data and calculating the impact of suggestions on user activity. Here's how we can approach it:\n",
    "\n",
    "Aggregating Step Data: We’ll aggregate step data by user, time intervals (e.g., daily or hourly), and then calculate step counts before and after suggestions.\n",
    "\n",
    "Impact of Suggestions: We’ll compute the difference in steps before and after each suggestion to assess how much the suggestions affect user activity.\n",
    "\n",
    "Plan:\n",
    "Aggregate the step data from JBSteps and GFSteps by user and time intervals.\n",
    "\n",
    "Merge the step data with the Suggestions dataset to analyze the steps before and after the suggestions.\n",
    "\n",
    "Calculate the difference in steps before and after the suggestion as a feature representing the impact of suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBc7Rs1h9XFk"
   },
   "outputs": [],
   "source": [
    "# Aggregating JBSteps data by user and hour\n",
    "# Convert 'steps.utime' to datetime format\n",
    "jbsteps_cleaned['steps.utime'] = pd.to_datetime(jbsteps_cleaned['steps.utime'], errors='coerce')\n",
    "\n",
    "# Now proceed with the aggregation by user and hour\n",
    "jbsteps_agg = jbsteps_cleaned.groupby([jbsteps_cleaned['user.index'], jbsteps_cleaned['steps.utime'].dt.hour]).agg(\n",
    "    total_steps_jb=('steps', 'sum')\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKSGd-he-Bca"
   },
   "outputs": [],
   "source": [
    "# Aggregating GFSteps data by user and hour\n",
    "# For GFSteps\n",
    "gfsteps_cleaned['steps.utime'] = pd.to_datetime(gfsteps_cleaned['steps.utime'], errors='coerce')\n",
    "\n",
    "# Aggregating GFSteps data by user and hour\n",
    "gfsteps_agg = gfsteps_cleaned.groupby([gfsteps_cleaned['user.index'], gfsteps_cleaned['steps.utime'].dt.hour]).agg(\n",
    "    total_steps_gf=('steps', 'sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQzCuVfD-DJR",
    "outputId": "7328392e-1379-4fc4-f8ea-3be248ca9a68"
   },
   "outputs": [],
   "source": [
    "# We merge based on user.index and hour (assume that the suggestion times are aligned with hours)\n",
    "suggestions_cleaned.loc[:, 'hour'] = pd.to_datetime(suggestions_cleaned['sugg.select.utime']).dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "htD6l7FT_Ooq",
    "outputId": "38ada3d4-19bf-45fc-85f5-cee2c7585dbb"
   },
   "outputs": [],
   "source": [
    "# Extract hour from the original jbsteps_cleaned dataset\n",
    "jbsteps_cleaned['hour'] = jbsteps_cleaned['steps.utime'].dt.hour\n",
    "\n",
    "# Aggregating JBSteps data by user and hour, summing the steps\n",
    "jbsteps_agg = jbsteps_cleaned.groupby(['user.index', 'hour']).agg(\n",
    "    total_steps_jb=('steps', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Display the updated jbsteps_agg with proper 'hour' values\n",
    "jbsteps_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "o46QuVxHARxS",
    "outputId": "5d71b90a-e24a-4da3-9a16-e2e8406d140d"
   },
   "outputs": [],
   "source": [
    "# Extract hour from the original gfsteps_cleaned dataset\n",
    "gfsteps_cleaned['hour'] = gfsteps_cleaned['steps.utime'].dt.hour\n",
    "\n",
    "# Aggregating GFSteps data by user and hour, summing the steps\n",
    "gfsteps_agg = gfsteps_cleaned.groupby(['user.index', 'hour']).agg(\n",
    "    total_steps_gf=('steps', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Display the updated gfsteps_agg with proper 'hour' values\n",
    "gfsteps_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8tducND_sJ7"
   },
   "outputs": [],
   "source": [
    "# Merge with JBSteps\n",
    "merged_data_jb = pd.merge(suggestions_cleaned, jbsteps_agg, how='left', on=['user.index', 'hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSI1hgbp_3cV"
   },
   "outputs": [],
   "source": [
    "# Merge with GFSteps\n",
    "merged_data = pd.merge(merged_data_jb, gfsteps_agg, how='left', on=['user.index', 'hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fsCnOadc_8f1",
    "outputId": "8c02ebc7-49cf-48d1-d508-0ca8ef6da8ec"
   },
   "outputs": [],
   "source": [
    "# Calculate the difference in steps using available step data\n",
    "# We'll use jbsteps60pre and gfsteps60pre for step counts before the suggestion (if they exist) and total_steps_jb and total_steps_gf for after.\n",
    "\n",
    "# Calculating step difference for Jawbone\n",
    "if 'jbsteps60pre' in merged_data.columns:\n",
    "    merged_data['step_diff_jb'] = merged_data['total_steps_jb'] - merged_data['jbsteps60pre']\n",
    "else:\n",
    "    merged_data['step_diff_jb'] = None\n",
    "\n",
    "# Calculating step difference for Google Fit\n",
    "if 'gfsteps60pre' in merged_data.columns:\n",
    "    merged_data['step_diff_gf'] = merged_data['total_steps_gf'] - merged_data['gfsteps60pre']\n",
    "else:\n",
    "    merged_data['step_diff_gf'] = None\n",
    "\n",
    "# Display the first few rows of the dataset with step differences\n",
    "merged_data[['user.index', 'step_diff_jb', 'step_diff_gf']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "qyjcxWlpEvgU",
    "outputId": "a37279f1-7e94-4d0e-ff4e-2443df3c3c88"
   },
   "outputs": [],
   "source": [
    "# Merge JBSteps (merged_data_jb) and Users datasets\n",
    "final_merged_data = pd.merge(merged_data_jb, users_cleaned, how='left', on='user.index')\n",
    "\n",
    "# Now, select relevant columns from Suggestions and Users datasets\n",
    "# Relevant columns from Users dataset (e.g., demographic and survey data)\n",
    "users_features_updated = [\n",
    "    'user.index', 'intake.survey.utime', 'exit.survey.utime',\n",
    "    'modact.metmins.exit', 'walk.time.exit', 'vigact.time.exit',\n",
    "    'sit.time.exit', 'ipaq.hepa.exit'\n",
    "]\n",
    "\n",
    "# Selecting relevant columns from Suggestions dataset (e.g., suggestion type, time slots, user context)\n",
    "suggestions_features = ['user.index', 'send.active', 'send.sedentary', 'sugg.select.slot', 'dec.temperature', 'dec.windspeed']\n",
    "\n",
    "# Extract these relevant columns for the final dataset\n",
    "final_selected_data = final_merged_data[suggestions_features + users_features_updated]\n",
    "\n",
    "# Display the first few rows of the final dataset with selected features\n",
    "final_selected_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWpuH0jxCaQQ"
   },
   "source": [
    "**Handling Missing Data:** We should handle the missing values in both step_diff_jb and step_diff_gf.\n",
    "\n",
    "**Fill missing values with a default value (e.g., 0 for no step change)**.\n",
    "\n",
    "**Drop rows with missing values.**\n",
    "\n",
    "**Normalization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wadkTxwYAbvf",
    "outputId": "3969aa99-9ea6-4f66-90d4-a405cc1bc08e"
   },
   "outputs": [],
   "source": [
    "# Handle missing data\n",
    "# We can fill missing values in step_diff_jb and step_diff_gf with 0 (indicating no step difference where data is missing)\n",
    "\n",
    "merged_data.loc[:, 'step_diff_jb'] = merged_data['step_diff_jb'].fillna(0)\n",
    "merged_data.loc[:, 'step_diff_gf'] = merged_data['step_diff_gf'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ueW7WxPTCtoJ",
    "outputId": "1b69c723-8a09-4069-97e2-103bbd1cf10c"
   },
   "outputs": [],
   "source": [
    "#Normalize the step differences for further analysis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalizing step differences (both Jawbone and Google Fit)\n",
    "merged_data[['step_diff_jb_norm', 'step_diff_gf_norm']] = scaler.fit_transform(merged_data[['step_diff_jb', 'step_diff_gf']])\n",
    "\n",
    "# Display the normalized columns\n",
    "merged_data[['user.index', 'step_diff_jb_norm', 'step_diff_gf_norm']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl2QkwQnHAly"
   },
   "source": [
    "**Statistical Analysis**.\n",
    "The goal here is to assess whether different types of suggestions (e.g., active vs. sedentary) lead to significant changes in user activity.\n",
    "\n",
    "Plan:\n",
    "\n",
    "**Hypothesis:** We test whether active suggestions lead to a significant increase in steps compared to sedentary suggestions.\n",
    "\n",
    "Statistical Tests:\n",
    "T-tests: Compare the mean step differences between groups (e.g., active vs. sedentary suggestions).\n",
    "\n",
    "ANOVA: If we want to test across multiple groups or suggestion types, we can use ANOVA.\n",
    "Implementation:\n",
    "Split the data based on the type of suggestion (active vs. sedentary).\n",
    "Run statistical tests (t-test) to check if the step difference after active suggestions is significantly higher than after sedentary suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "apF9VWU7DqH6",
    "outputId": "3cd2eb4d-39df-47ec-cd18-b6ef085e2ba4"
   },
   "outputs": [],
   "source": [
    "# Reinitialize the bell curve (normal distribution) data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# t-statistic and p-value from previous test\n",
    "t_stat = 1.15\n",
    "p_value = 0.250\n",
    "\n",
    "# Generate a bell curve (normal distribution)\n",
    "mean = 0\n",
    "std_dev = 1\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "y = norm.pdf(x, mean, std_dev)\n",
    "\n",
    "# Plot the bell curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label='Normal Distribution (t-distribution approximation)', color='blue')\n",
    "\n",
    "# Mark the t-statistic on the curve\n",
    "plt.axvline(x=t_stat, color='red', linestyle='--', label=f't-statistic: {t_stat:.2f}')\n",
    "\n",
    "# Fill the area under the curve for p-value (two-tailed)\n",
    "plt.fill_between(x, 0, y, where=(x <= -abs(t_stat)) | (x >= abs(t_stat)), color='red', alpha=0.3, label=f'p-value: {p_value:.3f}')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('T-Test Visualization: P-value on the Bell Curve')\n",
    "plt.xlabel('t-statistic using JBSteps & Suggestions Data')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZl_mIZCIPU0"
   },
   "source": [
    "Interpretation:\n",
    "The p-value of 0.213 suggests that the difference in step counts between active and sedentary suggestions is not statistically significant at typical significance levels (e.g., 0.05).\n",
    "This means that, based on the current data, we do not have strong evidence to conclude that active suggestions lead to significantly more steps than sedentary suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBu7bQW1PMi5",
    "outputId": "cbfa381e-49f5-4fab-803f-4cb3fcf18f0a"
   },
   "outputs": [],
   "source": [
    "# Check the column names in the merged_data\n",
    "print(merged_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqC58F4qNKqn",
    "outputId": "9abead6c-0810-4afd-d6ef-9f432e1a7964"
   },
   "outputs": [],
   "source": [
    "# Use the correct column names based on your dataset\n",
    "time_series_data = merged_data[['sugg.select.utime', 'total_steps_jb']]  # You can also use 'total_steps_gf' if needed\n",
    "\n",
    "# Convert 'sugg.select.utime' to datetime format\n",
    "time_series_data['sugg.select.utime'] = pd.to_datetime(time_series_data['sugg.select.utime'], errors='coerce')\n",
    "\n",
    "# Set 'sugg.select.utime' as the index and sort the data by time\n",
    "time_series_data.set_index('sugg.select.utime', inplace=True)\n",
    "time_series_data = time_series_data.sort_index()\n",
    "\n",
    "# Drop any missing values if necessary\n",
    "time_series_data.dropna(inplace=True)\n",
    "\n",
    "# Display the first few rows to check\n",
    "print(time_series_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4Tw8sUhNLqq",
    "outputId": "a49a2fc4-f50a-4e27-ec96-3310592567ed"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform the Augmented Dickey-Fuller test\n",
    "result = adfuller(time_series_data['total_steps_jb'])\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "\n",
    "# Interpretation: If p-value > 0.05, the series is non-stationary, and differencing may be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guuqvAxGPsO_",
    "outputId": "ca9aaff4-4132-47ef-e5db-3fc8131cf2c1"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit ARIMA model on the original data since differencing isn't needed\n",
    "model = ARIMA(time_series_data['total_steps_jb'], order=(1, 0, 1))  # Adjust p, d, q as needed\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Hnk8_NbQRD2",
    "outputId": "49acc836-a153-4c6c-c35b-b0d7141d6d80"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Make predictions\n",
    "predictions = model_fit.predict(start=0, end=len(time_series_data)-1, dynamic=False)\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = mean_squared_error(time_series_data['total_steps_jb'], predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(time_series_data['total_steps_jb'], predictions)\n",
    "\n",
    "print(f'MSE: {mse}, RMSE: {rmse}, MAE: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XIycZhLQcL2",
    "outputId": "fea145a4-60a1-4389-a936-4fb6cbc9173a"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the range for p, d, q\n",
    "p = d = q = range(0, 3)\n",
    "\n",
    "# Generate all possible combinations of p, d, q\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "best_mse, best_params = float(\"inf\"), None\n",
    "\n",
    "# Iterate through all combinations and find the best parameters\n",
    "for param in pdq:\n",
    "    try:\n",
    "        model = ARIMA(time_series_data['total_steps_jb_scaled'], order=param)\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # Make predictions\n",
    "        predictions_scaled = model_fit.predict(start=0, end=len(time_series_data)-1, dynamic=False)\n",
    "        predictions_scaled = predictions_scaled.values.reshape(-1, 1)\n",
    "        predictions = scaler.inverse_transform(predictions_scaled)\n",
    "\n",
    "        # Calculate the MSE\n",
    "        mse = mean_squared_error(time_series_data['total_steps_jb'], predictions)\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse, best_params = mse, param\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f'Best ARIMA parameters: {best_params} with MSE: {best_mse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ha381YLSHN_"
   },
   "source": [
    "ARIMA FAILED SEVERLY! So optimizing it with another model XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7JyItxxSRhi",
    "outputId": "2b88e0c8-188b-4b4e-e8c6-4d3f32a58d7f"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate timestamps from the time series data\n",
    "time_series_data = time_series_data.loc[~time_series_data.index.duplicated(keep='first')]\n",
    "\n",
    "# Now create the lag features\n",
    "def create_lag_features(data, lag=5):\n",
    "    lagged_data = pd.DataFrame()\n",
    "    for i in range(1, lag + 1):\n",
    "        lagged_data[f'lag_{i}'] = data['total_steps_jb'].shift(i)\n",
    "    return lagged_data\n",
    "\n",
    "# Create lagged features for total_steps_jb\n",
    "lagged_features = create_lag_features(time_series_data, lag=5)\n",
    "\n",
    "# Drop any missing values that were introduced by shifting\n",
    "lagged_features.dropna(inplace=True)\n",
    "\n",
    "# Add the current value (target) to the features\n",
    "lagged_features['target'] = time_series_data['total_steps_jb'][lagged_features.index]\n",
    "\n",
    "# Display the first few rows of the lagged dataset\n",
    "print(lagged_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl-itr54SW_Z"
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "split_index = int(len(lagged_features) * 0.8)  # 80% training, 20% testing\n",
    "\n",
    "train = lagged_features[:split_index]\n",
    "test = lagged_features[split_index:]\n",
    "\n",
    "# Split into features (X) and target (y)\n",
    "X_train = train.drop('target', axis=1)\n",
    "y_train = train['target']\n",
    "X_test = test.drop('target', axis=1)\n",
    "y_test = test['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xVJ_j7CWSkwf",
    "outputId": "2d7953e6-e2b3-4435-9438-ac9977631f75"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n",
    "test_dmatrix = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 5,\n",
    "    'eta': 0.1\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgboost_model = xgb.train(params, train_dmatrix, num_boost_round=100)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgboost_model.predict(test_dmatrix)\n",
    "\n",
    "# Display first few predictions\n",
    "print(y_pred[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6v4jyB4SnI2",
    "outputId": "f371d923-60e9-4aa3-ae48-233ded9606bd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the error metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'MSE: {mse}, RMSE: {rmse}, MAE: {mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "l30OtFHRSpRS",
    "outputId": "6b32e130-62e7-4e3a-af90-71c03d8ebbdb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],          # Controls the depth of each tree\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Step size shrinkage to prevent overfitting\n",
    "    'n_estimators': [100, 200, 300],     # Number of trees (boosting rounds)\n",
    "    'min_child_weight': [1, 3, 5],       # Minimum sum of instance weight needed in a child\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],         # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'subsample': [0.8, 1.0],             # Subsampling ratio of training instances\n",
    "    'colsample_bytree': [0.8, 1.0],      # Subsampling ratio of features for each tree\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Perform Grid Search with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best parameters to train the model again\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate error metrics\n",
    "mse_best = mean_squared_error(y_test, y_pred)\n",
    "rmse_best = np.sqrt(mse_best)\n",
    "mae_best = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Model MSE: {mse_best}, RMSE: {rmse_best}, MAE: {mae_best}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4uZDlZtqWHf4",
    "outputId": "4f5ee3d6-47c7-4728-926a-abfbfea2b79d"
   },
   "outputs": [],
   "source": [
    "# Create new time-based features from the timestamp ('sugg.select.utime')\n",
    "time_series_data['hour'] = time_series_data.index.hour  # Hour of the day\n",
    "time_series_data['day'] = time_series_data.index.day  # Day of the month\n",
    "time_series_data['dayofweek'] = time_series_data.index.dayofweek  # Day of the week (0=Monday, 6=Sunday)\n",
    "time_series_data['month'] = time_series_data.index.month  # Month of the year\n",
    "\n",
    "# Display the first few rows to see the new features\n",
    "print(time_series_data[['total_steps_jb', 'hour', 'day', 'dayofweek', 'month']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Qdpm4bN6eCxv",
    "outputId": "aafed748-3464-432b-b342-f9ae07a48b3a"
   },
   "outputs": [],
   "source": [
    "# Create lag features (e.g., steps from previous time steps)\n",
    "def create_lag_features(data, lag=5):\n",
    "    lagged_data = pd.DataFrame()\n",
    "    for i in range(1, lag + 1):\n",
    "        lagged_data[f'lag_{i}'] = data['total_steps_jb'].shift(i)\n",
    "    return lagged_data\n",
    "\n",
    "# Create lagged features\n",
    "lagged_features = create_lag_features(time_series_data, lag=5)\n",
    "\n",
    "# Combine lag features with the new time-based features\n",
    "lagged_features = pd.concat([lagged_features, time_series_data[['hour', 'day', 'dayofweek', 'month']]], axis=1)\n",
    "\n",
    "# Add the current value (target) to the features\n",
    "lagged_features['target'] = time_series_data['total_steps_jb'][lagged_features.index]\n",
    "\n",
    "# Drop rows with missing values caused by lagging\n",
    "lagged_features.dropna(inplace=True)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(lagged_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "HH3sro7feEj4"
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "split_index = int(len(lagged_features) * 0.8)  # 80% training, 20% testing\n",
    "\n",
    "train = lagged_features[:split_index]\n",
    "test = lagged_features[split_index:]\n",
    "\n",
    "# Split into features (X) and target (y)\n",
    "X_train = train.drop('target', axis=1)\n",
    "y_train = train['target']\n",
    "X_test = test.drop('target', axis=1)\n",
    "y_test = test['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "SNDFXs1ReGXS",
    "outputId": "9ceb8cd6-e0cb-4406-d185-598574a25c19"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n",
    "test_dmatrix = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 5,\n",
    "    'eta': 0.1\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgboost_model = xgb.train(params, train_dmatrix, num_boost_round=100)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgboost_model.predict(test_dmatrix)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(y_pred[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dwNBPxBLeIyR",
    "outputId": "7d77abac-6f02-41b7-cb65-d4e073038950"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'MSE: {mse}, RMSE: {rmse}, MAE: {mae}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (AML)",
   "language": "python",
   "name": "aml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
